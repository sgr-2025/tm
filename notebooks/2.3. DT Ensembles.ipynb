{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# ======================================================================================\n",
    "# Notebook setup\n",
    "# \n",
    "# Run this cell before all others to make sure that the Jupyter notebook works properly\n",
    "# ======================================================================================\n",
    "\n",
    "# Automatically reload all imported modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "figsize = (14, 3.5)\n",
    "\n",
    "data = pd.read_csv(os.path.join('..', 'data', 'real_estate.csv'), sep=',')\n",
    "cols = data.columns\n",
    "X = data[cols[:-1]]\n",
    "y = np.log(data[cols[-1]])\n",
    "X_tr, X_ts, y_tr, y_ts = train_test_split(X, y, test_size=0.34, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ensemble Models\n",
    "\n",
    "**We finished the earlier notebook with the following considerations**\n",
    "\n",
    "* DTs are non-linear models and quite prone to overfitting\n",
    "* This can be mitigated (e.g. by reducing depth), but often with a loss of accuracy\n",
    "* There is no easy fix by using a single DT\n",
    "\n",
    "**What about using _multiple_ DTs?**\n",
    "\n",
    "For example, we could think of training _several, different, DTs_\n",
    "\n",
    "* Each disting DT would risk overfitting, but _in a different way_\n",
    "* Intuitively, overfitting would then appear as _random noise_\n",
    "* ...Which could be canceled out via aggregation (averaging, voting)\n",
    "\n",
    "**This is the key idea behind _ensemble DT models_**\n",
    "\n",
    "We will (briefly) discuss a few of the main approach in this class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Random Forests\n",
    "\n",
    "**_Random Forests_ work exactly as we have described**\n",
    "\n",
    "We assume we built $m$ be different trees (estimators):\n",
    "\n",
    "* Let $f_{i}(x)$ be the output of the $i$-th tree\n",
    "* Let $\\mathbb{I}(f_{i}(x) = k)$ is the indicator function for $f_{i}(x) = k$\n",
    "* ...which is equal to 1 if $f_{i}(x) = k$ and 0 otherwise\n",
    "\n",
    "**Then the output of a RF for _classification_ is given by:**\n",
    "\n",
    "$$\n",
    "f(x) = \\text{argmax}_{k \\in K} \\sum_{i=1}^m \\mathbb{I}(f_i(x) = k)\n",
    "$$\n",
    "\n",
    "* Intuitively: every tree \"votes\" for a class\n",
    "* ...And we pick the class with the largest number of votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Random Forests\n",
    "\n",
    "**We can use Random Forests for regression, too**\n",
    "\n",
    "In this case the model output is just an average:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{m} \\sum_{i=1}^m f_i(x)\n",
    "$$\n",
    "\n",
    "**We obtain different trees**\n",
    "\n",
    "...By using slighly different daset to train them. In particular we use:\n",
    "\n",
    "* Bootstrapping: we select _random subsets of the examples_\n",
    "* Bagging: we select _random subsets of the attributes_\n",
    "\n",
    "**They are called Random Forests due to:**\n",
    "\n",
    "* The use of multiple trees\n",
    "* The introduction of random elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Random Forests\n",
    "\n",
    "**We can learn a Random Forest model as usual**\n",
    "\n",
    "First we build a model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfm = RandomForestRegressor(n_estimators=150, max_depth=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can specify the number of estimators (along the usual parameters)\n",
    "\n",
    "Then we train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm.fit(X_tr, y_tr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...And finally we obtain the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.967 (training), 0.743 (test)\n"
     ]
    }
   ],
   "source": [
    "rfm_pred_tr, rfm_pred_ts = rfm.predict(X_tr), rfm.predict(X_ts)\n",
    "print(f'R2: {r2_score(y_tr, rfm_pred_tr):.3} (training), {r2_score(y_ts, rfm_pred_ts):.3} (test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Random Forests\n",
    "\n",
    "**We can optimize the parameters using grid search and cross validation**\n",
    "\n",
    "This is interesting to check how RF differ from simple DTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': np.arange(50, 201, 50), 'max_depth': np.arange(2, 10, 2)}\n",
    "rfm_cv = GridSearchCV(rfm, param_grid=param_grid)\n",
    "rfm_cv.fit(X_tr, y_tr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not necessarily better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.954 (training), 0.741 (test)\n"
     ]
    }
   ],
   "source": [
    "rfm_cv_pred_tr, rfm_cv_pred_ts = rfm_cv.predict(X_tr), rfm_cv.predict(X_ts)\n",
    "print(f'R2: {r2_score(y_tr, rfm_cv_pred_tr):.3} (training), {r2_score(y_ts, rfm_cv_pred_ts):.3} (test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...But the optimal parameters tend to be _more reliable_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best results with: {'max_depth': 8, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best results with: {rfm_cv.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Random Forests\n",
    "\n",
    "**RF and DTs operate in a very different way**\n",
    "\n",
    "The basic ingredient is still a DT\n",
    "\n",
    "* DTs need to be deep in order to be expressive\n",
    "* ...But deeper DTs are also very prone to overfitting\n",
    "\n",
    "RFs have an innate mechanism to counter overfitting\n",
    "\n",
    "* ...And therefore they can afford being much deeper\n",
    "* This makes them much more reliable at learning complex relations\n",
    "\n",
    "They are a bit slower to train\n",
    "\n",
    "* ...But still fast compared to other complex ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient Boosted Trees\n",
    "\n",
    "**_Gradient Boosted Trees_ also employ multiple DTs, but in a different fashion**\n",
    "\n",
    "Technically, the output of a GBT model is just the sum of the individual ouputs\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^m f_i(x)\n",
    "$$\n",
    "\n",
    "* Where the notation is the same we used for random forests\n",
    "\n",
    "**However, the trees are trained sequentially:**\n",
    "\n",
    "* The first tree (i.e. $f_1(x)$) is trained as usual\n",
    "* The second tree is trained to apply a correction over $f_1(x)$\n",
    "* The third to apply a correction over $f_1(x) + f_2(x)$, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient Boosted Trees\n",
    "\n",
    "**The corrections are incremental:**\n",
    "\n",
    "* Each new tree does not try to completely fix the previous predictions\n",
    "* ...But rather to make a step in the right direction\n",
    "* ...As given by an exact or approximate _gradient_\n",
    "\n",
    "**There is no need for randomization, in principle:**\n",
    "\n",
    "* The incremental training process already yields different trees\n",
    "* However, the scikit-learn implementation has some random elements\n",
    "\n",
    "**The idea of using a \"chain\" of corrective models is general**\n",
    "\n",
    "* It is called _gradient boosting_ and can be applied with other model types\n",
    "* ...But GBTs are the probably the best known example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient Boosted Trees\n",
    "\n",
    "**We can learn Gradient Boosted Trees model as usual**\n",
    "\n",
    "First we build a model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbm = GradientBoostingRegressor(n_estimators=100, max_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can specify the number of estimators (along the usual parameters)\n",
    "\n",
    "Then we train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.fit(X_tr, y_tr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...And finally we obtain the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.944 (training), 0.724 (test)\n"
     ]
    }
   ],
   "source": [
    "gbm_pred_tr, gbm_pred_ts = gbm.predict(X_tr), gbm.predict(X_ts)\n",
    "print(f'R2: {r2_score(y_tr, gbm_pred_tr):.3} (training), {r2_score(y_ts, gbm_pred_ts):.3} (test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient Boosted Trees\n",
    "\n",
    "**We can optimize the parameters using grid search and cross validation**\n",
    "\n",
    "This is interesting to check how GBTs differ from RFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'n_estimators': np.arange(50, 200, 50), 'max_depth': np.arange(2, 10, 2)}\n",
    "gbm_cv = GridSearchCV(gbm, param_grid=param_grid)\n",
    "gbm_cv.fit(X_tr, y_tr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results should be more or less comparable with RFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.95 (training), 0.748 (test)\n"
     ]
    }
   ],
   "source": [
    "gbm_cv_pred_tr, gbm_cv_pred_ts = gbm_cv.predict(X_tr), gbm_cv.predict(X_ts)\n",
    "print(f'R2: {r2_score(y_tr, gbm_cv_pred_tr):.3} (training), {r2_score(y_ts, gbm_cv_pred_ts):.3} (test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...But the optimal are very different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best results with: {'max_depth': 4, 'n_estimators': 50}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best results with: {gbm_cv.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient Boosted Trees\n",
    "\n",
    "**GBTs use multiple trees, like RFs, but operate very differently**\n",
    "\n",
    "Random Forests need to be deep in order to be expressive\n",
    "\n",
    "* Since all trees trying to learn the same input-output relation\n",
    "* Depth is needed to capture complexity\n",
    "\n",
    "Gradient Boosted Trees have an innate mechanism (summation) to:\n",
    "\n",
    "* Counter overfitting\n",
    "* _and_ improve expressivity\n",
    "\n",
    "As a result, individual trees in GBT can be much shallower\n",
    "\n",
    "* Typically shallow trees tend to work better\n",
    "\n",
    "**What works best between RFs and GBTs is application-dependent**\n",
    "\n",
    "...But both are among the best ML models for tabular datasets!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "rise": {
   "center": false,
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
